{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4.3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup your imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pull the training set from the newsgroup data\n",
    "The data has 20 different categories. Try to shrink down to smaller number of groups according to the definition here:\n",
    "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train',\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7532"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data_train.target\n",
    "x = data_train.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mappy = {\n",
    "    0: [0],\n",
    "    1: [1,2,3,4,5],\n",
    "    2: [6],\n",
    "    3: [7,8,9,10],\n",
    "    4: [11,12,13,14],\n",
    "    5: [15],\n",
    "    6: [16,17,18,19]\n",
    "}\n",
    "\n",
    "def getkey(num):\n",
    "    for x, y in mappy.items():\n",
    "        if num in y:\n",
    "            return x\n",
    "y_new = [getkey(num) for num in y]\n",
    "y = y_new\n",
    "labels = ['alt','comp','misc','rec','sci','soc','talk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create the vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "                        max_features=1000,\n",
    "                        token_pattern='[a-zA-Z]{3,50}',\n",
    "                        ngram_range=(1, 1), \n",
    "                        max_df=0.1, \n",
    "                        min_df=0.0001,\n",
    "                        stop_words='english',\n",
    ")\n",
    "\n",
    "x_ = tfidf.fit_transform(x)\n",
    "xtfidf = tfidf.get_feature_names()\n",
    "\n",
    "# X_all = pd.DataFrame(x_.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are top 50 most powerful terms in deciding news groups? (hint: treat it as a classification problem)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "selector = SelectKBest(f_classif, k=50)\n",
    "selected_data = selector.fit_transform(X_all, y)\n",
    "kbest_columns = X_all.columns[selector.get_support()]\n",
    "Xbest = pd.DataFrame(selected_data, columns=kbest_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'asking', u'atheism', u'atheist', u'atheists', u'belief', u'bible',\n",
       "       u'bike', u'car', u'card', u'catholic', u'christ', u'christian',\n",
       "       u'christianity', u'christians', u'church', u'clipper', u'condition',\n",
       "       u'dos', u'encryption', u'faith', u'file', u'game', u'god',\n",
       "       u'government', u'graphics', u'gun', u'heaven', u'hockey', u'israel',\n",
       "       u'israeli', u'jesus', u'league', u'manuals', u'offer', u'people',\n",
       "       u'players', u'religion', u'resurrection', u'sale', u'scripture',\n",
       "       u'season', u'sell', u'shipping', u'sin', u'software', u'space', u'team',\n",
       "       u'thanks', u'using', u'windows'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kbest_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create the Truncated Singular Value Decomposition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = TfidfVectorizer(\n",
    "                        max_features=100,\n",
    "                        token_pattern='[a-zA-Z]{3,50}',\n",
    "                        ngram_range=(1, 1), \n",
    "                        max_df=0.1, \n",
    "                        min_df=0.0001,\n",
    "                        stop_words='english')\n",
    "X_V = v.fit_transform(x)\n",
    "\n",
    "v_cols = v.get_feature_names()\n",
    "\n",
    "# X_all = pd.DataFrame(x_.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(X_V.todense(), columns = v.get_feature_names())\n",
    "# v_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01329341  0.02474308  0.0218014   0.01956765  0.01624408  0.01593683\n",
      "  0.01577902  0.01516742  0.01484366  0.0143997   0.01420579  0.01397389\n",
      "  0.01347224  0.0129579   0.01283648  0.01274955  0.01262152  0.01247336\n",
      "  0.01236118  0.01217708  0.01193953  0.01163752  0.01160214  0.01153023\n",
      "  0.01129159  0.01121802  0.01117062  0.01104169  0.01092708  0.01073319\n",
      "  0.01067812  0.01058831  0.01056545  0.01047611  0.0103895   0.01033147\n",
      "  0.01030787  0.01022153  0.01012416  0.0100339   0.00993493  0.00988739\n",
      "  0.00983315  0.00979431  0.00977031  0.00968884  0.00964164  0.00956819\n",
      "  0.00955474  0.00946544  0.00940647  0.00934441  0.00927374  0.00920649\n",
      "  0.00912334  0.009104    0.00908779  0.00904201  0.00895932  0.00895198\n",
      "  0.008835    0.00879819  0.00870057  0.00863383  0.00853014  0.00836084\n",
      "  0.0083096   0.00821943  0.00808298  0.00798678  0.00791606  0.00783853]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.80726371651033679"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=72, n_iter=7, random_state=42)\n",
    "svd.fit(X_V)\n",
    "\n",
    "print svd.explained_variance_ratio_\n",
    "svd.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(svd.components_).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.index = v_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "did           0.187502\n",
       "edu           0.184342\n",
       "really        0.166639\n",
       "problem       0.164431\n",
       "god           0.151855\n",
       "work          0.150915\n",
       "said          0.149788\n",
       "years         0.145207\n",
       "going         0.140322\n",
       "believe       0.140164\n",
       "year          0.138929\n",
       "got           0.132601\n",
       "sure          0.130477\n",
       "point         0.128790\n",
       "using         0.126926\n",
       "things        0.126024\n",
       "better        0.124803\n",
       "help          0.118180\n",
       "thing         0.117927\n",
       "let           0.117805\n",
       "doesn         0.116295\n",
       "question      0.114439\n",
       "government    0.113466\n",
       "long          0.113300\n",
       "case          0.111921\n",
       "probably      0.111649\n",
       "didn          0.111414\n",
       "read          0.110286\n",
       "little        0.109906\n",
       "mail          0.109052\n",
       "                ...   \n",
       "called        0.076540\n",
       "data          0.076359\n",
       "end           0.075945\n",
       "line          0.074795\n",
       "problems      0.074374\n",
       "group         0.074318\n",
       "support       0.073284\n",
       "space         0.073225\n",
       "key           0.072822\n",
       "non           0.072774\n",
       "card          0.072473\n",
       "order         0.072078\n",
       "team          0.071859\n",
       "email         0.069057\n",
       "example       0.068262\n",
       "file          0.068108\n",
       "following     0.067748\n",
       "software      0.067575\n",
       "send          0.067055\n",
       "list          0.066804\n",
       "based         0.064529\n",
       "computer      0.063863\n",
       "public        0.062934\n",
       "jesus         0.057105\n",
       "version       0.056895\n",
       "window        0.048825\n",
       "files         0.047423\n",
       "dos           0.042921\n",
       "ftp           0.033649\n",
       "max           0.010615\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Setup your k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=7, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = svd.transform(X_V)\n",
    "\n",
    "from sklearn import cluster\n",
    "k = 7\n",
    "kmeans = cluster.KMeans(n_clusters=k)\n",
    "kmeans.fit(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 2, ..., 1, 6, 6], dtype=int32)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = kmeans.labels_\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058428812960829142"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.silhouette_score(X_new,labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fit the vectorizer and SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are top 50 most useful terms based on article itself? Are those terms similar to the top 50 from step 2? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cummulative variace from the terms. x-axis: number of components; y-axis: cummulative variance. \n",
    "Based on the plot, decide how many principle components you need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Fit the kmeans (Question: in this case, do you recommend running K-means without dimension reduction?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Print out your centroids. Look at the value for each centroid. Does each centroid represent a news group as expected? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Check the performance of our kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.114\n",
      "Completeness: 0.138\n",
      "V-measure: 0.124\n",
      "Adjusted Rand-Index: 0.045\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(y, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(y, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.19      0.17      0.18       480\n",
      "          1       0.49      0.07      0.13      2936\n",
      "          2       0.02      0.12      0.04       585\n",
      "          3       0.01      0.00      0.01      2389\n",
      "          4       0.15      0.04      0.06      2373\n",
      "          5       0.00      0.00      0.00       599\n",
      "          6       0.13      0.36      0.19      1952\n",
      "\n",
      "avg / total       0.19      0.10      0.09     11314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix. Hint: create a map to translate the label between k-means clustering and the original target (newsgroups_train.target). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  82,   11,  183,    2,   27,    2,  173],\n",
       "       [   3,  219,  260,  560,   57,  331, 1506],\n",
       "       [   1,    9,   68,   35,    4,   99,  369],\n",
       "       [  16,   55,  765,    8,  165,  236, 1144],\n",
       "       [   6,   92,  792,   30,   89,   27, 1337],\n",
       "       [ 238,   16,  172,    0,   31,    0,  142],\n",
       "       [  87,   43,  893,    5,  203,   13,  708]])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Repeat the lab with:\n",
    "- varying values of \"k\" \n",
    "- trying a different way to pick starting centroids ('k-means++' is the default method for centroids). For example, pick one point from each newsgroup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Learnt that feature engineering is more important. The scores for this lab are not great as the words for each news group is not selected. TFIDVECTORIZER creates features for all common words, but does not provide any weitage per news group. Distance between words for each news group is not obvious and hence the scores are bad. Due to lack of time, I will not spend more time on feature engineering for now. This Lab just helps in undertstanding the steps involved to create features from texts and then use trancated SVD to reduce dimensions, and then apply on Kme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
